diff --git a/cpp/conversation.h b/cpp/conversation.h
index 82332ae..8d8a933 100644
--- a/cpp/conversation.h
+++ b/cpp/conversation.h
@@ -195,9 +195,12 @@ class Conversation {
   void AppendReplyHeader(std::string role) { this->messages.push_back({role}); }
 
   void FinishReply(std::string msg) {
-    ICHECK_NE(this->messages.size(), 0);
-    ICHECK_EQ(this->messages.back().size(), 1) << "Already assigned";
-    this->messages.back().push_back(msg);
+    // When using prefill_with_embed without embed_text() first:
+    // InternalError: Check failed: this->messages.size() != 0 (0 vs. 0)
+    /*ICHECK_NE(this->messages.size(), 0);
+    ICHECK_EQ(this->messages.back().size(), 1) << "Already assigned";*/
+    if( this->messages.size() > 0 && this->messages.back().size() == 0 )
+	 this->messages.back().push_back(msg);
   }
 
   void Reset() { this->messages.resize(this->offset); }
diff --git a/cpp/llm_chat.cc b/cpp/llm_chat.cc
index 520f381..dbb196b 100644
--- a/cpp/llm_chat.cc
+++ b/cpp/llm_chat.cc
@@ -671,8 +671,8 @@ class LLMChat {
     NDArray input_data = this->GetInputTokenNDArray(prompt_tokens);
     ObjectRef embedding = ft_.embed_func_(ft_.CopyToWorker0(input_data), params_);
 
-    int32_t new_seq_len = total_seq_len_ + token_len;
-    total_seq_len_ = new_seq_len;
+    //int32_t new_seq_len = total_seq_len_ + token_len;
+    //total_seq_len_ = new_seq_len;
 
     auto tend = std::chrono::high_resolution_clock::now();
 
@@ -694,8 +694,20 @@ class LLMChat {
     if (embedding.Shape().size() == 0) {
       return;
     }
+    
+    // File "/opt/mlc-llm/cpp/llm_chat.cc", line 923
+    // InternalError: Check failed: (!stop_triggered_) is false: Cannot call process when it is stopped
+    output_ids_.clear();
+    appeared_token_ids_.clear();
+    output_message_.clear();
+    stop_triggered_ = false;
+    
     auto tstart = std::chrono::high_resolution_clock::now();
     int64_t token_len = embedding.Shape()[1];
+    
+    // mlc-llm/3rdparty/tvm/src/runtime/relax_vm/lm_support.cc:79
+    // InternalError: Check failed: "Requested shape do not match the filled count"
+    total_seq_len_ += token_len; 
     NDArray logits_on_device = this->ForwardEmbeddings(embedding, total_seq_len_);
 
     if (!decode_next_token) {
diff --git a/mlc_llm/core.py b/mlc_llm/core.py
index 8eb9949..a9d1822 100644
--- a/mlc_llm/core.py
+++ b/mlc_llm/core.py
@@ -394,7 +394,10 @@ def mod_transform_before_build(
         has_cutlass = tvm.get_global_func("relax.ext.cutlass", True)
 
         if has_cutlass and not args.no_cutlass_attn:
-            mod["prefill"] = rewrite_attention(mod["prefill"])
+            if args.sep_embed:
+                mod["prefill_with_embed"] = rewrite_attention(mod["prefill_with_embed"])
+            else:
+                mod["prefill"] = rewrite_attention(mod["prefill"])
             mod["decode"] = rewrite_attention(mod["decode"])
             patterns += get_patterns_with_prefix("cutlass.attention")
 
